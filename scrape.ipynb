{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ff4dc76f-a541-4969-ab3a-7b9ea77629c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART A: DOWNLOAD ALL OF ELECTIONLINE WEEKLY\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import os\n",
    "\n",
    "# YEARS = reversed(range(2011, 2024))\n",
    "\n",
    "# # Define user-agent to simulate a web browser request\n",
    "# headers = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "# }\n",
    "\n",
    "# base_url = 'https://electionline.org'\n",
    "\n",
    "# for year in YEARS:\n",
    "#     dir_path = f\"electionline-weekly/{year}\"\n",
    "#     os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "#     url = f\"{base_url}/electionline-weekly/{year}\"\n",
    "    \n",
    "#     # Send an HTTP GET request with headers\n",
    "#     response = requests.get(url, headers=headers)\n",
    "\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "#     weeks = soup.find('ul', class_='weeks').find_all('li')\n",
    "#     weeks = [f\"{base_url}{week.find('a')['href']}\" for week in weeks]\n",
    "\n",
    "#     for week in weeks:\n",
    "#         response = requests.get(week, headers=headers)\n",
    "#         with open(f\"{week.split('electionline.org/')[-1]}.html\", 'w') as f:\n",
    "#             f.write(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5657fdc0-996a-4d0a-a4d1-5b32717d5961",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.jp-OutputArea-output {display:flex}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# enable horizontal scroll of data frame in Jupyter lab\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.jp-OutputArea-output {display:flex}</style>\"))\n",
    "\n",
    "pd.options.display.html.use_mathjax = False\n",
    "pd.options.display.min_rows = 100\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "# go through all the saved HTML, and make each\n",
    "# paragraph a row in a pandas data frame.\n",
    "# use regex to pull out key details\n",
    "\n",
    "YEARS = reversed(range(2011, 2024))\n",
    "\n",
    "job_df = pd.DataFrame()\n",
    "\n",
    "for year in YEARS:\n",
    "    dir = f\"electionline-weekly/{year}\"\n",
    "    weeks = os.listdir(dir)\n",
    "\n",
    "    for week in weeks:\n",
    "        date = week[:5]\n",
    "        \n",
    "        with open(os.path.join(dir, week)) as f:\n",
    "            text = f.read()\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "    \n",
    "        # Find all divs with the class 'article-wrapper'\n",
    "        divs_with_class = soup.find_all('div', class_='article-wrapper')\n",
    "    \n",
    "    \n",
    "        for div in divs_with_class:\n",
    "            h2_tags = div.find_all('h2', string=re.compile(r'^job', re.I))\n",
    "            \n",
    "            if h2_tags:\n",
    "                for h2_tag in h2_tags:\n",
    "                    # Find all p elements within the div containing the matched h2 tag\n",
    "                    # Skip the first paragraph\n",
    "                    job_paragraphs = div.find_all('p')[1:]  \n",
    "\n",
    "                    # Skip intro and empty paragraphs\n",
    "                    job_paragraphs = [para for para in job_paragraphs if (not para.text.startswith('electionlineWeekly')) and (len(para.text)>10)]    \n",
    "                    \n",
    "                    for paragraph in job_paragraphs:\n",
    "                        # Extract job information from the paragraph\n",
    "                        link = paragraph.find('a')\n",
    "                        job_title = link.get_text() if link is not None else \"\"\n",
    "                        \n",
    "                        employer_match = re.search(r'\\/a>[^,]*(?:,|-)\\s*([^-–—]*)', str(paragraph))\n",
    "                        employer = employer_match.group(1) if employer_match else \"\"\n",
    "                        \n",
    "                        # salary_match = re.search(r'Salary[^:]*:\\s(.*?)(?=\\.\\s)', paragraph.get_text())\n",
    "                        salary_match = re.search(r'Salary[^:]*:\\s(.*?)(?=\\.\\s|Dead)', paragraph.get_text())\n",
    "                        salary = salary_match.group(1) if salary_match else \"\"\n",
    "                        \n",
    "                        description = paragraph.get_text()\n",
    "                        job_link = link['href'] if link is not None else \"\"\n",
    "    \n",
    "                        # Append job information to the list as a dictionary\n",
    "                        new_row = pd.DataFrame({'Job Title': job_title,\n",
    "                                                'Employer': employer,\n",
    "                                                'Salary': salary,\n",
    "                                                'Description': description,\n",
    "                                                'Link': job_link,\n",
    "                                                'Date': date,\n",
    "                                                'Year': year}, index=[0])\n",
    "                        job_df = pd.concat([job_df, new_row], ignore_index=True)\n",
    "\n",
    "# some cleanup!\n",
    "\n",
    "job_df = job_df.drop_duplicates(subset=['Job Title', 'Employer', 'Salary', 'Link'], keep='last')\n",
    "\n",
    "# exclude listings from some of the top URLs belonging to private employers\n",
    "excluded_domains = ['dominionvoting.com',\n",
    "                   'clearballot.com',\n",
    "                   'electioninnovation.org',\n",
    "                   'runbeck.net',\n",
    "                   'rockthevote.com',\n",
    "                   'hartintercivic.com',\n",
    "                   'fordfoundation.org',\n",
    "                   'techandciviclife.org',\n",
    "                   'bipartisanpolicy.org',\n",
    "                   'cdt.org',\n",
    "                   'ericstates.org',\n",
    "                   'centerfortechandciviclife.recruitee.com',\n",
    "                   'democracy.works',\n",
    "                   'electionreformers.org',\n",
    "                   'verifiedvoting.org']\n",
    "\n",
    "def is_not_excluded_domain(url):\n",
    "    netloc = urlparse(url).netloc.replace('www.', '')\n",
    "    return netloc not in excluded_domains\n",
    "\n",
    "job_df = job_df[job_df['Link'].apply(is_not_excluded_domain)]\n",
    "\n",
    "def pay_basis(x):\n",
    "    x = x.lower()\n",
    "    if re.search('hr|hour', x):\n",
    "        return 'hourly'\n",
    "    elif re.search('month', x):\n",
    "        return 'monthly'\n",
    "    elif re.search('biweek', x):\n",
    "        return 'biweekly'\n",
    "    elif re.search('week', x):\n",
    "        return 'weekly'\n",
    "    else:\n",
    "        return 'yearly'\n",
    "\n",
    "job_df['Pay basis'] = job_df['Salary'].apply(pay_basis)\n",
    "\n",
    " # just a guess to correct the typo. It could be 110, or 101,... not sure.\n",
    "job_df.loc[(job_df['Job Title']=='Elections Manager') & (job_df['Employer'] == 'Anoka County, Minnesota'), 'Salary']='$88,628-$101,878'\n",
    "\n",
    "def clean_salary(x):\n",
    "    cleaned_string = x.replace(' to ', '-').replace(' and ', '-')\n",
    "    cleaned_string = cleaned_string.replace('Grade 14-', '')\n",
    "    cleaned_string = re.sub(f'[^0-9\\.\\-–—]', '', cleaned_string) # get rid of things that aren't dashes and hyphens, periods, en, or em dashes.\n",
    "    return cleaned_string.replace('–', '-').replace('—', '-') # replace en and em dashes with hyphens\n",
    "\n",
    "job_df['Clean salary'] = job_df['Salary'].apply(clean_salary)\n",
    "\n",
    "def split_salary(x, end='low'):\n",
    "    # pattern = r'[-–—]'  # Matches hyphen, en dash, or em dash\n",
    "    pattern = '-'\n",
    "    ends = [part.strip().strip('$').strip('.') for part in re.split(pattern, x)]\n",
    "    if end=='low':\n",
    "        return ends[0]\n",
    "    else:\n",
    "        if len(ends) > 1:\n",
    "            return ends[1]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "job_df['Salary low end'] = pd.to_numeric(job_df['Clean salary'].apply(lambda x: split_salary(x, end='low')))\n",
    "job_df['Salary high end'] = pd.to_numeric(job_df['Clean salary'].apply(lambda x: split_salary(x, end='high')))\n",
    "\n",
    "# todo: replace K with 000. if it's under 100, assume hourly.\n",
    "\n",
    "\n",
    "# yearly = job_df[job_df['Pay basis']=='yearly']\n",
    "\n",
    "\n",
    "# job_df.sort_values('Salary high end', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9027ae53-d715-4e4c-8559-f645de914195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scrapeghost to have GPT-3.5 pull\n",
    "# out features in the data. it will probably\n",
    "# be better than my regex.\n",
    "\n",
    "suffix = '_sg'\n",
    "schema = {\n",
    "        \"job_title\": \"string\",\n",
    "        \"employer\": \"string\",\n",
    "        \"state_full_name\": \"string\",\n",
    "        \"salary_low_end\": \"float\",\n",
    "        \"salary_high_end\": \"float\",\n",
    "        \"pay_basis\": \"yearly, monthly, hourly, etc.\",\n",
    "                 }\n",
    "scrape_job_description = SchemaScraper(schema=schema)\n",
    "\n",
    "schema_renamed = {f'{key}{suffix}': value for key, value in schema.items()}\n",
    "job_df[list(schema_renamed.keys())] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0b4c1857-ba11-4cd0-b376-4d21c9e71dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6973"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7c6a169a-5779-4a35-9bfe-7910818bdb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 70/70 [01:41<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "cost = 0\n",
    "\n",
    "# extra_rows = []\n",
    "\n",
    "# row = 6973\n",
    "for row in tqdm(job_df.loc[row:].index):\n",
    "    description = job_df.loc[row]['Description']\n",
    "    response = scrape_job_description(description)\n",
    "    if isinstance(response.data, list): # rarely, it will be a list because multiple jobs are in one paragraph\n",
    "        extra_rows += response.data\n",
    "    else: # vast majority of rows\n",
    "        data = {f'{key}{suffix}': value for key, value in response.data.items()}\n",
    "        job_df.loc[row, data.keys()] = data.values()\n",
    "\n",
    "    cost += response.total_cost\n",
    "# print(f\"Running Cost: ${cost:.4f}\")\n",
    "\n",
    "extra_df = pd.DataFrame(extra_rows)\n",
    "extra_df.columns = list(schema_renamed.keys())\n",
    "job_df = pd.concat([job_df, extra_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "382968b5-8612-4dc3-9ef5-7384a326347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some cleanup\n",
    "\n",
    "job_df['pay_basis_sg'] = job_df['pay_basis_sg'].str.lower()\n",
    "\n",
    "yearly_synonyms = ['salary', 'annually']\n",
    "for syn in yearly_synonyms:\n",
    "    job_df['pay_basis_sg'] = job_df['pay_basis_sg'].str.replace(syn, 'yearly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "013d2618-03e4-4bef-80ef-399a28a19051",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df.to_pickle('jobs.pickle')\n",
    "job_df.to_csv('jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a91760f2-82b9-4d6b-b1a5-01f7a68ec213",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yearly_w_pay = job_df[job_df['pay_basis_sg']=='yearly']\n",
    "len(yearly_w_pay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3eb70e8e-b48b-48a9-9005-4edecadaf556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state_full_name_sg\n",
       "North Carolina                                87\n",
       "California                                    79\n",
       "Colorado                                      79\n",
       "Virginia                                      59\n",
       "Washington                                    54\n",
       "Arizona                                       48\n",
       "                                              45\n",
       "Oregon                                        34\n",
       "Michigan                                      27\n",
       "Florida                                       24\n",
       "Texas                                         24\n",
       "Maryland                                      22\n",
       "Georgia                                       21\n",
       "Illinois                                      17\n",
       "Ohio                                          15\n",
       "Minnesota                                     14\n",
       "Idaho                                         10\n",
       "Washington, D.C.                               9\n",
       "Pennsylvania                                   8\n",
       "Nevada                                         7\n",
       "Wisconsin                                      6\n",
       "New Mexico                                     6\n",
       "South Carolina                                 5\n",
       "Rhode Island                                   5\n",
       "Connecticut                                    5\n",
       "New York                                       4\n",
       "Utah                                           4\n",
       "North Dakota                                   4\n",
       "Missouri                                       3\n",
       "Iowa                                           3\n",
       "Denver                                         3\n",
       "Alabama                                        2\n",
       "Orange County, Florida                         2\n",
       "District of Columbia                           2\n",
       "Fulton County, Ga.                             2\n",
       "N/A                                            2\n",
       "Montana                                        2\n",
       "Montgomery County, Md.                         1\n",
       "Los Angeles                                    1\n",
       "Massachusetts                                  1\n",
       "NCSL’s Redistricting and Elections program     1\n",
       "Alaska                                         1\n",
       "Wyoming                                        1\n",
       "Kansas                                         1\n",
       "King County Department of Elections            1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df[job_df['salary_low_end_sg'].notna()]['state_full_name_sg'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "88b33407-2571-4332-b510-05781d02f9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state_full_name_sg\n",
       "Virginia                                      58\n",
       "North Carolina                                56\n",
       "California                                    55\n",
       "Colorado                                      42\n",
       "                                              37\n",
       "Arizona                                       33\n",
       "Florida                                       24\n",
       "Maryland                                      22\n",
       "Georgia                                       21\n",
       "Washington                                    18\n",
       "Michigan                                      17\n",
       "Oregon                                        14\n",
       "Texas                                         13\n",
       "Ohio                                          13\n",
       "Minnesota                                     11\n",
       "Illinois                                       8\n",
       "Washington, D.C.                               8\n",
       "Pennsylvania                                   7\n",
       "New Mexico                                     6\n",
       "Nevada                                         5\n",
       "South Carolina                                 5\n",
       "Rhode Island                                   5\n",
       "Connecticut                                    5\n",
       "New York                                       4\n",
       "Wisconsin                                      4\n",
       "Idaho                                          4\n",
       "Utah                                           3\n",
       "Iowa                                           3\n",
       "Alabama                                        2\n",
       "Fulton County, Ga.                             2\n",
       "North Dakota                                   2\n",
       "Denver                                         2\n",
       "District of Columbia                           2\n",
       "Orange County, Florida                         2\n",
       "N/A                                            2\n",
       "Missouri                                       2\n",
       "Montana                                        1\n",
       "Massachusetts                                  1\n",
       "Montgomery County, Md.                         1\n",
       "NCSL’s Redistricting and Elections program     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yearly_w_pay['state_full_name_sg'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
